#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Author: Luca Gioacchini

"""
TannerParser is a subclass of HoneypotParser for parsing log files generated by 
the Tanner honeypot. It provides methods for extracting labels related to 
crawlers and zombie-log4j attacks from the log data.
"""

from .parser import HoneypotParser
import gzip
from io import BytesIO
import json

class TannerParser(HoneypotParser):
    def __init__(self, filepath, outpath=None):
        """
        A subclass of HoneypotParser for parsing log files generated by the 
        Tanner honeypot. This subclass implements specific logic for extracting 
        labels related to crawlers and zombie-log4j attacks from the log data.

        Parameters
        ----------
        filepath : str
            The file path of the log file to be parsed.
        outpath : str, optional
            The file path to save the extracted labels to. If not provided, 
            labels will not be saved to a file.

        Methods
        -------
        load_log_file(filepath)
            Load the log file and return a list of log entries as strings.
        extract_labels()
            Extract labels related to crawlers and zombie-log4j attacks from 
            the log entries.
        _extract_crawler_label(label1, label2)
            Extract labels related to crawlers from the log entries.
        _extract_zombie_log4j_label(label1, label2)
            Extract labels related to zombie-log4j attacks from the log entries.

        """
        super().__init__(filepath, outpath)

    def load_log_file(self, filepath):
        """
        Load the log file and return a list of log entries as strings.

        Parameters
        ----------
        filepath : str
            The file path of the log file to be loaded.

        Returns
        -------
        list
            A list of log entries as strings.

        """
        # Read the bytes object into a file-like object
        with gzip.open(filepath, 'rb') as f_in:
            file_like_obj = BytesIO(f_in.read())

        # Convert the data in the file-like object to a string
        data_string = file_like_obj.getvalue()

        # Decode the string using the utf-8 character encoding and split the
        # rows
        data_string = data_string.decode('utf-8')
        logs = data_string.split('\n')

        return logs

    def _extract_crawler_label(self, label1='benign', label2='crawler'):
        """
        Extract labels related to crawlers from the log entries.

        Parameters
        ----------
        label1 : str, optional
            The first label to use for the extracted IP addresses. The default
            value is 'benign'.
        label2 : str, optional
            The second label to use for the extracted IP addresses. The default
            value is 'crawler'.

        Returns
        -------
        list
            A list of tuples, where each tuple contains the extracted IP
            address and labels for a single log entry.

        """
        # Extract the IP addresses of crawlers from the logs
        crawlers = []
        for entry in self.logs:
            try:
                # Check if the entry contains 'robots.txt'
                if 'robots.txt' in entry:
                    # Parse the entry as JSON and extract the source IP address
                    obj = json.loads(entry)
                    src_ip = obj['peer']['ip']

                    # Add the IP address to the list of crawlers if it's not 
                    # '10.0.0.1'
                    if src_ip != '10.0.0.1':
                        label = (src_ip, label1, label2, f'unk_{label2}')
                        # Get only unique senders
                        if label not in crawlers:
                            crawlers.append(label)
            except:
                # Skip the entry if it can't be parsed as JSON
                continue

        return crawlers      

    def _extract_zombie_log4j_label(self, label1, label2):
        """
        Extract labels related to zombie-log4j attacks from the log entries.

        Parameters
        ----------
        label1 : str
            The first label to use for the extracted IP addresses. The default
            value is 'malicious'.
        label2 : str, optional
            The second label to use for the extracted IP addresses. The default
            value is 'zombie'.

        Returns
        -------
        list
            A list of tuples, where each tuple contains the extracted IP
            address and labels for a single log entry.

        """
        label1 = label1 or 'malicious'
        label2 = label2 or 'zombie'

        # Extract the IP addresses from the logs
        log4j = []
        for entry in self.logs:
            # Check if the entry contains 'robots.txt'
            if 'jndi:ldap' in entry:
                # Parse the entry as JSON and extract the source IP address
                obj = json.loads(entry)
                src_ip = obj['peer']['ip']

                # Add the IP address to the list of crawlers if it's not 
                # '10.0.0.1'
                if src_ip != '10.0.0.1':
                    label = (src_ip, label1, label2, 'log4j')
                    # Get only unique senders
                    if label not in log4j:
                        log4j.append(label)

        return log4j

    def extract_labels(self):
        """
        Extract labels related to crawlers and zombie-log4j attacks from the log
        entries. If an output file path has been specified, save the labels to
        the file.

        Returns
        -------
        list
            A list of tuples, where each tuple contains the extracted IP
            address and labels for a single log entry.

        """
        # Extract the IP addresses labeled as crawler from the log data
        crawler_ips = self._extract_crawler_label()
        # Extract the IP addresses labeled as zombie-log4j from the log data
        log4j_ips = self._extract_zombie_log4j_label()
        
        # Concatenate labels
        tanner_all = crawler_ips + log4j_ips

        # If an output file path has been specified, save the labels to file
        if self.outpath:
            self.save_labels(tanner_all)

        # Return the list of tuples containing the IP addresses and labels
        return tanner_all